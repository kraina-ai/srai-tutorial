{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training hex2vec model\n",
    "\n",
    "This notebook shows step-by-step how to pre-train larger hex2vec model using SRAI library "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection\n",
    "\n",
    "This example works on Polish cities, with 50k+ inhibitants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\n",
    "    \"Warszawa, Polska\",\n",
    "    \"Kraków, Polska\",\n",
    "    \"Łódź, Polska\",\n",
    "    \"Wrocław, Polska\",\n",
    "    \"Poznań, Polska\",\n",
    "    \"Gdańsk, Polska\",\n",
    "    \"Szczecin, Polska\",\n",
    "    \"Bydgoszcz, Polska\",\n",
    "    \"Lublin, Polska\",\n",
    "    \"Białystok, Polska\",\n",
    "    \"Katowice, Polska\",\n",
    "    \"Gdynia, Polska\",\n",
    "    \"Częstochowa, Polska\",\n",
    "    \"Radom, Polska\",\n",
    "    \"Toruń, Polska\",\n",
    "    \"Rzeszów, Polska\",\n",
    "    \"Sosnowiec, Polska\",\n",
    "    \"Kielce, Polska\",\n",
    "    \"Gliwice, Polska\",\n",
    "    \"Olsztyn, Polska\",\n",
    "    \"Zabrze, Polska\",\n",
    "    \"Bielsko-Biała, Polska\",\n",
    "    \"Bytom, Polska\",\n",
    "    \"Zielona Góra, Polska\",\n",
    "    \"Rybnik, Polska\",\n",
    "    \"Ruda Śląska, Polska\",\n",
    "    \"Opole, Polska\",\n",
    "    \"Tychy, Polska\",\n",
    "    \"Gorzów Wielkopolski, Polska\",\n",
    "    \"Elbląg, Polska\",\n",
    "    \"Dąbrowa Górnicza, Polska\",\n",
    "    \"Płock, Polska\",\n",
    "    \"Wałbrzych, Polska\",\n",
    "    \"Włocławek, Polska\",\n",
    "    \"Tarnów, Polska\",\n",
    "    \"Chorzów, Polska\",\n",
    "    \"Koszalin, Polska\",\n",
    "    ##50k\n",
    "    \"Kalisz, Polska\",\n",
    "    \"Legnica, Polska\",\n",
    "    \"Grudziądz, Polska\",\n",
    "    \"Jaworzno, Polska\",\n",
    "    \"Słupsk, Polska\",\n",
    "    \"Jastrzębie-Zdrój, Polska\",\n",
    "    \"Nowy Sącz, Polska\",\n",
    "    \"Jelenia Góra, Polska\",\n",
    "    \"Siedlce, Polska\",\n",
    "    \"Mysłowice, Polska\",\n",
    "    \"Konin, Polska\",\n",
    "    \"Piła, Polska\",\n",
    "    \"Piotrków Trybunalski, Polska\",\n",
    "    \"Lubin, Polska\",\n",
    "    \"Inowrocław, Polska\",\n",
    "    \"Ostrów Wielkopolski, Polska\",\n",
    "    \"Suwałki, Polska\",\n",
    "    \"Stargard, Polska\",\n",
    "    \"Gniezno, Polska\",\n",
    "    \"Ostrowiec Świętokrzyski, Polska\",\n",
    "    \"Siemianowice Śląskie, Polska\",\n",
    "    \"Głogów, Polska\",\n",
    "    \"Pabianice, Polska\",\n",
    "    \"Leszno, Polska\",\n",
    "    \"Żory, Polska\",\n",
    "    \"Zamość, Polska\",\n",
    "    \"Pruszków, Polska\",\n",
    "    \"Łomża, Polska\",\n",
    "    \"Ełk, Polska\",\n",
    "    \"Tarnowskie Góry, Polska\",\n",
    "    \"Tomaszów Mazowiecki, Polska\",\n",
    "    \"Chełm, Polska\",\n",
    "    \"Mielec, Polska\",\n",
    "    \"Kędzierzyn-Koźle, Polska\",\n",
    "    \"Przemyśl, Polska\",\n",
    "    \"Stalowa Wola, Polska\",\n",
    "    \"Tczew, Polska\",\n",
    "    \"Biała Podlaska, Polska\",\n",
    "    \"Bełchatów, Polska\",\n",
    "    \"Świdnica, Polska\",\n",
    "    \"Będzin, Polska\",\n",
    "    \"Zgierz, Polska\",\n",
    "    \"Piekary Śląskie, Polska\",\n",
    "    \"Racibórz, Polska\",\n",
    "    \"Legionowo, Polska\",\n",
    "    \"Ostrołęka, Polska\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_people = [\n",
    "    1794166,\n",
    "    779966,\n",
    "    672185,\n",
    "    641928,\n",
    "    532048,\n",
    "    470805,\n",
    "    398255,\n",
    "    344091,\n",
    "    338586,\n",
    "    296958,\n",
    "    290553,\n",
    "    244969,\n",
    "    217530,\n",
    "    209296,\n",
    "    198613,\n",
    "    197863,\n",
    "    197586,\n",
    "    193415,\n",
    "    177049,\n",
    "    171249,\n",
    "    170924,\n",
    "    169756,\n",
    "    163255,\n",
    "    140892,\n",
    "    137128,\n",
    "    136423,\n",
    "    127839,\n",
    "    126871,\n",
    "    122589,\n",
    "    118582,\n",
    "    118285,\n",
    "    118268,\n",
    "    109971,\n",
    "    108561,\n",
    "    107498,\n",
    "    106846,\n",
    "    106235,\n",
    "    99106,\n",
    "    98436,\n",
    "    93564,\n",
    "    90368,\n",
    "    89780,\n",
    "    88038,\n",
    "    83558,\n",
    "    78335,\n",
    "    77813,\n",
    "    74559,\n",
    "    72539,\n",
    "    72527,\n",
    "    72250,\n",
    "    71710,\n",
    "    71674,\n",
    "    71560,\n",
    "    69639,\n",
    "    67579,\n",
    "    67570,\n",
    "    67404,\n",
    "    66270,\n",
    "    66120,\n",
    "    63945,\n",
    "    62854,\n",
    "    62844,\n",
    "    62785,\n",
    "    62623,\n",
    "    62573,\n",
    "    61903,\n",
    "    61756,\n",
    "    61338,\n",
    "    61135,\n",
    "    60075,\n",
    "    60021,\n",
    "    59779,\n",
    "    59623,\n",
    "    59430,\n",
    "    56942,\n",
    "    56419,\n",
    "    56222,\n",
    "    56008,\n",
    "    55673,\n",
    "    54702,\n",
    "    54259,\n",
    "    53529,\n",
    "    51656,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select h3 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to use train/test split by cites. This approach ensures that cities are evently distributed based on their size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"city\": cities, \"num_people\": num_people})\n",
    "def get_stratify_index(num_people) -> int:\n",
    "    if num_people >= 500_000:\n",
    "        return 0\n",
    "    elif num_people >= 200_000:\n",
    "        return 1\n",
    "    elif num_people >= 100_000:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "df[\"stratify\"] = df[\"num_people\"].apply(get_stratify_index)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.regionalizers import H3Regionalizer\n",
    "from tqdm.auto import tqdm\n",
    "from srai.regionalizers import geocode_to_region_gdf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading cities boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = [\n",
    "    geocode_to_region_gdf(city) for city in tqdm(cities)\n",
    "]\n",
    "pd.concat(areas).explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split cities into H3 regions and create train and val datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from typing import List\n",
    "\n",
    "def get_regions(cities: List[str], resolution: int) -> gpd.GeoDataFrame:\n",
    "    areas = [\n",
    "        geocode_to_region_gdf(city) for city in tqdm(cities)\n",
    "    ]\n",
    "    regionizer = H3Regionalizer(resolution=resolution)\n",
    "    regions_gdf = regionizer.transform(pd.concat(areas))\n",
    "    return regions_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"stratify\"])\n",
    "train_df = train_df.sort_values(by=\"num_people\", ascending=False)\n",
    "val_df = val_df.sort_values(by=\"num_people\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cities = train_df[\"city\"].tolist()\n",
    "val_cities = val_df[\"city\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_regions_gdf = get_regions(train_cities, resolution=RESOLUTION)\n",
    "val_regions_gdf = get_regions(val_cities, resolution=RESOLUTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hex2vec training requires neighbourhood information, we use SRAI implementation of this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.neighbourhoods import H3Neighbourhood\n",
    "\n",
    "train_neighbourhood = H3Neighbourhood(train_regions_gdf)\n",
    "val_neighbourhood = H3Neighbourhood(val_regions_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading OSM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.loaders.osm_loaders.filters import HEX2VEC_FILTER\n",
    "from srai.embedders.hex2vec import Hex2VecEmbedder\n",
    "from functional import seq\n",
    "\n",
    "\n",
    "expected_output_features = seq(HEX2VEC_FILTER.items()).starmap(lambda k, v: list(map(lambda x: f\"{k}_{x}\", v))).flatten().list()\n",
    "expected_output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes long time, so we save features to `parquet` files and load them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.loaders import OSMPbfLoader\n",
    "from pathlib import Path\n",
    "\n",
    "save_dir = Path(\"data/raw\").resolve()\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "loader = OSMPbfLoader()\n",
    "\n",
    "for city in cities:\n",
    "    features = loader.load(geocode_to_region_gdf(city), HEX2VEC_FILTER)\n",
    "    features.to_parquet(save_dir / f\"{city}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(cities: List[str]) -> gpd.GeoDataFrame:\n",
    "    features = [\n",
    "        gpd.read_parquet(f\"data/raw/{city}.parquet\") for city in tqdm(cities)\n",
    "    ]\n",
    "    features = pd.concat(features)\n",
    "    features = features[~features.index.duplicated(keep='first')]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = get_features(train_cities)\n",
    "val_features = get_features(val_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_regions_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathing features with H3 regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.joiners import IntersectionJoiner\n",
    "\n",
    "joiner = IntersectionJoiner()\n",
    "train_joint = joiner.transform(train_regions_gdf, train_features)\n",
    "val_joint = joiner.transform(val_regions_gdf, val_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = [256, 128, 64]\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import wandb\n",
    "from srai.embedders import Hex2VecEmbedder\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=2, monitor=\"val_f1\", mode=\"max\", filename=\"{epoch}-{val_f1:.2f}\", dirpath=f\"models_{RESOLUTION}/{model_size}\")\n",
    "embedder = Hex2VecEmbedder(encoder_sizes=model_size, expected_output_features=expected_output_features)\n",
    "logger = WandbLogger(project=f\"hex2vec_pl_r{RESOLUTION}_b1024_50k\", name=f\"model_sizes={model_size}\")\n",
    "embedder.fit_transform(train_regions_gdf, train_features, train_joint, train_neighbourhood, val_regions_gdf, val_features, val_joint, val_neighbourhood, batch_size=1024, trainer_kwargs={\"max_epochs\": EPOCHS, \"accelerator\": \"gpu\", \"logger\": logger, \"callbacks\": [checkpoint_callback]})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And done ;) We have succesfully pre-trained our model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
