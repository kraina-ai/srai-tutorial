{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kjQhO3W4MFp"
      },
      "outputs": [],
      "source": [
        "%pip install srai[all] seaborn lonboard optuna optuna-integration[lightgbm] lightgbm datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D39bbRSGquA"
      },
      "source": [
        "# Predict Airbnb listing price\n",
        "\n",
        "In this notebook we will try to predict the price of the AirBnB listings in Paris using geospatial embeddings.\n",
        "\n",
        "We will create a spatial stratification logic to divide objects to be in different parts of the city.\n",
        "\n",
        "Later, we will use Optuna to optimize parameters for a given LightGBM model and compare results for multiple embeddings.\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/kraina-ai/srai-tutorial/blob/geopython2025/tutorial/03_airbnb_rental_price_prediction.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Airbnb dataset\n",
        "\n",
        "Data from: https://zenodo.org/records/4446043#.ZEV8d-zMI-R\n",
        "\n",
        "We will download the data from the HuggingFace hub, transform it to a GeoDataFrame and subselect features to make model smaller for tutorial purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hhlyaYKCgh7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import geopandas as gpd\n",
        "import lightgbm as lgb\n",
        "import lonboard.basemap\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "from h3 import latlng_to_cell\n",
        "from lonboard import Map, ScatterplotLayer, PolygonLayer\n",
        "from lonboard.colormap import apply_continuous_cmap\n",
        "from optuna.integration import LightGBMPruningCallback\n",
        "from optuna.visualization import (\n",
        "    plot_contour,\n",
        "    plot_intermediate_values,\n",
        "    plot_optimization_history,\n",
        "    plot_param_importances,\n",
        ")\n",
        "from quackosm._osm_tags_filters import merge_osm_tags_filter\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error\n",
        "from srai.embedders import (\n",
        "    ContextualCountEmbedder,\n",
        "    CountEmbedder,\n",
        "    Hex2VecEmbedder,\n",
        ")\n",
        "from srai.h3 import h3_to_geoseries, ring_buffer_h3_regions_gdf\n",
        "from srai.joiners import IntersectionJoiner\n",
        "from srai.loaders import OSMPbfLoader\n",
        "from srai.loaders.osm_loaders.filters import GEOFABRIK_LAYERS\n",
        "from srai.neighbourhoods import H3Neighbourhood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Precalculate the OpenStreetMap extracts index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from quackosm.osm_extracts import _get_combined_index\n",
        "\n",
        "_get_combined_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the data from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jOZBeySW-5S"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"kraina/airbnb\", \"all\").filter(\n",
        "    lambda listing: listing[\"city\"] == \"Paris\"\n",
        ")\n",
        "raw_data = ds[\"train\"].to_pandas()\n",
        "\n",
        "listings_gdf = gpd.GeoDataFrame(\n",
        "    raw_data, geometry=gpd.points_from_xy(raw_data.lng, raw_data.lat), crs=\"EPSG:4326\"\n",
        ")\n",
        "listings_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckc28zkOvTbW"
      },
      "outputs": [],
      "source": [
        "target_column = \"realSum\"\n",
        "geometry_column = \"geometry\"\n",
        "feature_columns = [\n",
        "    \"person_capacity\",\n",
        "    \"cleanliness_rating\",\n",
        "    \"guest_satisfaction_overall\",\n",
        "    \"bedrooms\",\n",
        "    \"room_type\",\n",
        "]\n",
        "reduced_columns = [\n",
        "    target_column,\n",
        "    *feature_columns,\n",
        "    geometry_column,\n",
        "]\n",
        "listings_gdf = listings_gdf[reduced_columns]\n",
        "listings_gdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to transform categorical feature to one-hot encoding with `get_dummies` from pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkeaXa2iJId5"
      },
      "outputs": [],
      "source": [
        "categorical_columns = [\"room_type\"]\n",
        "\n",
        "for col in categorical_columns:\n",
        "    listings_gdf = pd.concat(\n",
        "        [listings_gdf, pd.get_dummies(listings_gdf[col], prefix=col)], axis=1\n",
        "    ).drop(col, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilGlWg17w8wP"
      },
      "outputs": [],
      "source": [
        "listings_gdf.iloc[1].to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see the distribution of the listings prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFhkZ1QpNPbP"
      },
      "outputs": [],
      "source": [
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "sns.histplot(listings_gdf, x=target_column, kde=True, ax=ax1)\n",
        "sns.histplot(\n",
        "    listings_gdf[\n",
        "        listings_gdf[target_column] <= listings_gdf[target_column].quantile(0.95)\n",
        "    ],\n",
        "    x=target_column,\n",
        "    kde=True,\n",
        "    ax=ax2,\n",
        ")\n",
        "\n",
        "ax1.set_title(\"Distribution of prices\")\n",
        "ax2.set_title(\"Zoomed in to <95th percentile\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And listings on the map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mhyhd-L6Susi"
      },
      "outputs": [],
      "source": [
        "filtered_listings_gdf = listings_gdf[\n",
        "    listings_gdf[target_column] <= listings_gdf[target_column].quantile(0.95)\n",
        "][[geometry_column, target_column]]\n",
        "\n",
        "layer = ScatterplotLayer.from_geopandas(\n",
        "    filtered_listings_gdf,\n",
        "    get_fill_color=apply_continuous_cmap(\n",
        "        filtered_listings_gdf[target_column]\n",
        "        / filtered_listings_gdf[target_column].max(),\n",
        "        mpl.colormaps[\"RdYlBu_r\"],\n",
        "        alpha=0.4,\n",
        "    ),\n",
        "    radius_units=\"meters\",\n",
        "    radius_min_pixels=1,\n",
        "    get_radius=filtered_listings_gdf[target_column] / 10,\n",
        ")\n",
        "\n",
        "m = Map(\n",
        "    layer, _height=800, basemap_style=lonboard.basemap.CartoBasemap.DarkMatterNoLabels\n",
        ")\n",
        "\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split the data spatially"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SPLITTING_H3_RESOLUTION = 8\n",
        "h3_splitting_column = \"h3_splitting\"\n",
        "\n",
        "listings_gdf[h3_splitting_column] = listings_gdf[geometry_column].apply(\n",
        "    lambda pt: latlng_to_cell(pt.y, pt.x, SPLITTING_H3_RESOLUTION)\n",
        ")\n",
        "listings_gdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Group the listings statistics per H3 cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "listing_statistics_per_h3 = listings_gdf.reset_index().groupby(h3_splitting_column).agg(\n",
        "    {\"index\": \"count\", \"realSum\": \"mean\"}\n",
        ").rename(columns={\"index\": \"listings\", \"realSum\": \"avg_price\"})\n",
        "listing_statistics_per_h3[\"quantile\"] = pd.qcut(\n",
        "    listing_statistics_per_h3[\"avg_price\"], 5, labels=False\n",
        ")\n",
        "listing_statistics_per_h3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split the dataset using these statistics\n",
        "\n",
        "We will use quantiles to stratify the regions and have both cheap and expensive regions in both groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7q3J-xf2tVX"
      },
      "outputs": [],
      "source": [
        "def split_h3_cells(ratio=0.2, tolerance=1e-2, random_state=42):\n",
        "    bucket_1 = []\n",
        "    bucket_2 = []\n",
        "\n",
        "    for quantile in listing_statistics_per_h3[\"quantile\"].unique():\n",
        "        _sub_bucket_1 = []\n",
        "        _sub_bucket_2 = []\n",
        "        current_listings = listing_statistics_per_h3[\n",
        "            listing_statistics_per_h3[\"quantile\"] == quantile\n",
        "        ].sample(\n",
        "            frac=1, random_state=random_state\n",
        "        )\n",
        "\n",
        "        for index, row in current_listings.iterrows():\n",
        "            current_bucket_2_sum = sum(t[1] for t in _sub_bucket_2)\n",
        "            current_ratio = current_bucket_2_sum / current_listings[\"listings\"].sum()\n",
        "\n",
        "            if current_ratio > ratio:\n",
        "                _sub_bucket_1.append((index, row[\"listings\"]))\n",
        "                continue\n",
        "\n",
        "            next_ratio = (current_bucket_2_sum + row[\"listings\"]) / current_listings[\n",
        "                \"listings\"\n",
        "            ].sum()\n",
        "            if next_ratio > (ratio * (1 + tolerance)):\n",
        "                _sub_bucket_1.append((index, row[\"listings\"]))\n",
        "                continue\n",
        "\n",
        "            if len(_sub_bucket_1) > len(_sub_bucket_2):\n",
        "                _sub_bucket_2.append((index, row[\"listings\"]))\n",
        "            else:\n",
        "                _sub_bucket_1.append((index, row[\"listings\"]))\n",
        "\n",
        "        bucket_1.extend(_sub_bucket_1)\n",
        "        bucket_2.extend(_sub_bucket_2)\n",
        "\n",
        "    bucket_2_total_listings = sum(t[1] for t in bucket_2)\n",
        "    total_listings = listing_statistics_per_h3[\"listings\"].sum()\n",
        "    print(\n",
        "        f\"Ratio: {(bucket_2_total_listings / total_listings):.3f} (expected: {ratio})\"\n",
        "    )\n",
        "\n",
        "    h3_bucket_1 = [t[0] for t in bucket_1]\n",
        "    h3_bucket_2 = [t[0] for t in bucket_2]\n",
        "\n",
        "    bucket_1_listings = listing_statistics_per_h3.loc[h3_bucket_1]\n",
        "    bucket_2_listings = listing_statistics_per_h3.loc[h3_bucket_2]\n",
        "\n",
        "    bucket_1_mean_value = (\n",
        "        bucket_1_listings[\"listings\"] * bucket_1_listings[\"avg_price\"]\n",
        "    ).sum() / bucket_1_listings[\"listings\"].sum()\n",
        "    bucket_2_mean_value = (\n",
        "        bucket_2_listings[\"listings\"] * bucket_2_listings[\"avg_price\"]\n",
        "    ).sum() / bucket_2_listings[\"listings\"].sum()\n",
        "\n",
        "    print(\n",
        "        f\"Mean listing prices: {(bucket_1_mean_value):.3f} / {(bucket_2_mean_value):.3f}\"\n",
        "    )\n",
        "\n",
        "    return h3_bucket_1, h3_bucket_2\n",
        "\n",
        "\n",
        "train_h3_cells, test_h3_cells = split_h3_cells(ratio=0.33, random_state=73)\n",
        "\n",
        "train_listing_ids = listings_gdf[listings_gdf[h3_splitting_column].isin(train_h3_cells)].index\n",
        "test_listing_ids = listings_gdf[listings_gdf[h3_splitting_column].isin(test_h3_cells)].index\n",
        "len(train_listing_ids), len(test_listing_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the splitted data on a map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFYC2UYN9jpR"
      },
      "outputs": [],
      "source": [
        "m = h3_to_geoseries(\n",
        "    listings_gdf.loc[train_listing_ids, h3_splitting_column].unique()\n",
        ").explore(tiles=\"CartoDB Positron\", color=\"royalblue\")\n",
        "h3_to_geoseries(\n",
        "    listings_gdf.loc[test_listing_ids, h3_splitting_column].unique()\n",
        ").explore(m=m, color=\"orange\")\n",
        "filtered_listings_gdf.explore(target_column, m=m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model\n",
        "\n",
        "Let's prepare some code to train the LigthGBM model and optimize the hyperparameters using Optuna.\n",
        "\n",
        "It will automatically run a study, evaluate the model trained with best parameters and display some charts with results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQsbIxnpz3Zk"
      },
      "outputs": [],
      "source": [
        "def find_best_lightgbm_model(\n",
        "    features, target, previous_best_params=None, n_trials=None, timeout=120\n",
        "):\n",
        "    x_train = features.loc[train_listing_ids]\n",
        "    x_test = features.loc[test_listing_ids]\n",
        "    y_train = target.loc[train_listing_ids]\n",
        "    y_test = target.loc[test_listing_ids]\n",
        "\n",
        "    train_dataset = lgb.Dataset(x_train, label=y_train)\n",
        "    eval_dataset = lgb.Dataset(x_test, label=y_test)\n",
        "\n",
        "    # https://github.com/optuna/optuna-examples/blob/main/lightgbm/lightgbm_integration.py\n",
        "    def objective(trial):\n",
        "        param = {\n",
        "            \"objective\": \"regression\",\n",
        "            \"metric\": [\"l1\", \"l2\"],\n",
        "            \"verbosity\": -1,\n",
        "            \"boosting_type\": \"gbdt\",\n",
        "            \"feature_pre_filter\": False,\n",
        "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
        "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
        "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
        "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
        "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
        "        }\n",
        "\n",
        "        warnings.filterwarnings(\n",
        "            \"ignore\",\n",
        "            category=optuna.exceptions.ExperimentalWarning,\n",
        "        )\n",
        "\n",
        "        pruning_callback = LightGBMPruningCallback(trial, \"l1\")\n",
        "\n",
        "        gbm = lgb.train(\n",
        "            param,\n",
        "            train_dataset,\n",
        "            valid_sets=[eval_dataset],\n",
        "            num_boost_round=200,\n",
        "            callbacks=[\n",
        "                pruning_callback,\n",
        "                lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        preds = gbm.predict(x_test, num_iteration=gbm.best_iteration + 1)\n",
        "        pred_labels = np.rint(preds)\n",
        "\n",
        "        rmse = root_mean_squared_error(y_test, pred_labels)\n",
        "        return rmse\n",
        "\n",
        "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "    study = optuna.create_study(\n",
        "        pruner=optuna.pruners.MedianPruner(\n",
        "            n_warmup_steps=5, n_min_trials=5, n_startup_trials=5\n",
        "        ),\n",
        "        direction=\"minimize\",\n",
        "    )\n",
        "\n",
        "    if previous_best_params is not None:\n",
        "        study.enqueue_trial(previous_best_params)\n",
        "\n",
        "    study.optimize(\n",
        "        objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(f\"  Value: {trial.value}\")\n",
        "    print(f\"  Params: {trial.params}\")\n",
        "\n",
        "    # Visualize the optimization history.\n",
        "    plot_optimization_history(study).show()\n",
        "\n",
        "    # Visualize the learning curves of the trials.\n",
        "    plot_intermediate_values(study).show()\n",
        "\n",
        "    # Visualize hyperparameter relationships.\n",
        "    plot_contour(study).show()\n",
        "\n",
        "    # Visualize parameter importances.\n",
        "    plot_param_importances(study).show()\n",
        "\n",
        "    gbm = lgb.train(\n",
        "        trial.params,\n",
        "        train_dataset,\n",
        "        valid_sets=[eval_dataset],\n",
        "        num_boost_round=10_000,\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=100)],\n",
        "    )\n",
        "\n",
        "    y_pred = gbm.predict(x_test)\n",
        "\n",
        "    result = dict(\n",
        "        RMSE=root_mean_squared_error(y_test, y_pred),\n",
        "        R2=r2_score(y_test, y_pred),\n",
        "        MAE=mean_absolute_error(y_test, y_pred),\n",
        "        n_features=features.shape[1],\n",
        "        y_test=y_test,\n",
        "        y_pred_test=y_pred,\n",
        "        y_pred_all=gbm.predict(features),\n",
        "    )\n",
        "\n",
        "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    for ax in (ax1, ax2):\n",
        "        sns.regplot(\n",
        "            x=y_test,\n",
        "            y=y_pred,\n",
        "            scatter_kws=dict(alpha=1, s=2),\n",
        "            line_kws=dict(color=\".2\", linestyle=\"--\"),\n",
        "            ax=ax,\n",
        "        )\n",
        "        sns.lineplot(x=[0, y_test.max()], y=[0, y_test.max()], color=\"red\", ax=ax)\n",
        "        ax.set_xlabel(\"Actual price\")\n",
        "        ax.set_ylabel(\"Predicted price\")\n",
        "\n",
        "    q95 = y_test.quantile(0.95)\n",
        "    ax2.set_xlim(0, q95)\n",
        "    ax2.set_ylim(0, q95)\n",
        "\n",
        "    ax1.set_title(\"Baseline model performance\")\n",
        "    ax2.set_title(\"Zoomed in to <95th percentile\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    lgb.plot_importance(gbm, max_num_features=20)\n",
        "    plt.show()\n",
        "\n",
        "    print(\n",
        "        f\"RMSE: {result['RMSE']:.2f}, MAE: {result['MAE']:.2f}, R2: {result['R2']:.2f}\"\n",
        "    )\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yqbHWOkDODy"
      },
      "source": [
        "### Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76JWt1fwV9s5"
      },
      "outputs": [],
      "source": [
        "results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9z0Pl1WG5bU"
      },
      "outputs": [],
      "source": [
        "features = listings_gdf.drop(\n",
        "    columns=[target_column, geometry_column, h3_splitting_column]\n",
        ")\n",
        "target = listings_gdf[target_column]\n",
        "\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iaca6eHa3dxT"
      },
      "outputs": [],
      "source": [
        "optuna_result = find_best_lightgbm_model(features, target, timeout=120)\n",
        "optuna_result[\"experiment\"] = \"baseline\"\n",
        "\n",
        "results.append(optuna_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q85wiqBpDQ1Z"
      },
      "source": [
        "### Count Embedder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOE-ECF-WrTr"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_H3_RESOLUTION = 10\n",
        "h3_embedding_column = \"h3_embedding\"\n",
        "\n",
        "listings_gdf[h3_embedding_column] = listings_gdf[geometry_column].apply(\n",
        "    lambda pt: latlng_to_cell(pt.y, pt.x, EMBEDDING_H3_RESOLUTION)\n",
        ")\n",
        "listings_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJcellQGXSRL"
      },
      "outputs": [],
      "source": [
        "unique_h3_cells = listings_gdf[h3_embedding_column].unique()\n",
        "regions_gdf = gpd.GeoDataFrame(\n",
        "    data=dict(region_id=unique_h3_cells), geometry=h3_to_geoseries(unique_h3_cells)\n",
        ").set_index(\"region_id\")\n",
        "regions_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGhpu91kCYI0"
      },
      "outputs": [],
      "source": [
        "flat_geofabrik_osm_tags_filter = merge_osm_tags_filter(GEOFABRIK_LAYERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3LqA9uBYCrG"
      },
      "outputs": [],
      "source": [
        "features_gdf = OSMPbfLoader().load(regions_gdf, flat_geofabrik_osm_tags_filter)\n",
        "features_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4itdJytzYqDF"
      },
      "outputs": [],
      "source": [
        "joint = IntersectionJoiner().transform(regions_gdf, features_gdf)\n",
        "joint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwzrjrxEYsCg"
      },
      "outputs": [],
      "source": [
        "count_embeddings = CountEmbedder(\n",
        "    expected_output_features=flat_geofabrik_osm_tags_filter, count_subcategories=True\n",
        ").transform(regions_gdf, features_gdf, joint)\n",
        "count_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bopdVJs7Y_4h"
      },
      "outputs": [],
      "source": [
        "features = listings_gdf.merge(\n",
        "    count_embeddings.reset_index(),\n",
        "    left_on=h3_embedding_column,\n",
        "    right_on=count_embeddings.index.name,\n",
        ").drop(\n",
        "    columns=[\n",
        "        target_column,\n",
        "        geometry_column,\n",
        "        h3_splitting_column,\n",
        "        h3_embedding_column,\n",
        "        count_embeddings.index.name,\n",
        "    ]\n",
        ")\n",
        "target = listings_gdf[target_column]\n",
        "\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IROEL8FAdyD"
      },
      "outputs": [],
      "source": [
        "optuna_result = find_best_lightgbm_model(features, target, timeout=120)\n",
        "optuna_result[\"experiment\"] = \"count embeddings\"\n",
        "\n",
        "results.append(optuna_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8s_eSceEUfW"
      },
      "source": [
        "### Contextual Count Embedder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGlZgJ3iZ3Si"
      },
      "outputs": [],
      "source": [
        "H3_NEIGHBOURHOOD = 5\n",
        "\n",
        "buffered_regions_gdf = ring_buffer_h3_regions_gdf(\n",
        "    regions_gdf, distance=H3_NEIGHBOURHOOD\n",
        ")\n",
        "buffered_regions_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F17uoX10aDdc"
      },
      "outputs": [],
      "source": [
        "features_gdf = OSMPbfLoader().load(buffered_regions_gdf, flat_geofabrik_osm_tags_filter)\n",
        "features_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkYUS4KYaHN6"
      },
      "outputs": [],
      "source": [
        "joint = IntersectionJoiner().transform(buffered_regions_gdf, features_gdf)\n",
        "joint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4LiePefaKU1"
      },
      "outputs": [],
      "source": [
        "context_embeddings = ContextualCountEmbedder(\n",
        "    count_subcategories=True,\n",
        "    expected_output_features=flat_geofabrik_osm_tags_filter,\n",
        "    neighbourhood=H3Neighbourhood(),\n",
        "    neighbourhood_distance=H3_NEIGHBOURHOOD,\n",
        "    concatenate_vectors=False,\n",
        ").transform(buffered_regions_gdf, features_gdf, joint)\n",
        "context_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPN9VkZrchfw"
      },
      "outputs": [],
      "source": [
        "features = listings_gdf.merge(\n",
        "    context_embeddings.reset_index(),\n",
        "    left_on=h3_embedding_column,\n",
        "    right_on=context_embeddings.index.name,\n",
        ").drop(\n",
        "    columns=[\n",
        "        target_column,\n",
        "        geometry_column,\n",
        "        h3_splitting_column,\n",
        "        h3_embedding_column,\n",
        "        context_embeddings.index.name,\n",
        "    ]\n",
        ")\n",
        "target = listings_gdf[target_column]\n",
        "\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCVcQoJfAa-b"
      },
      "outputs": [],
      "source": [
        "optuna_result = find_best_lightgbm_model(features, target, timeout=120)\n",
        "optuna_result[\"experiment\"] = \"contextual count embeddings\"\n",
        "\n",
        "results.append(optuna_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suxpJ4-oEhpQ"
      },
      "source": [
        "### Hex2Vec Embedder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i1vNZa4r-tZ"
      },
      "outputs": [],
      "source": [
        "features_gdf = OSMPbfLoader().load(buffered_regions_gdf, flat_geofabrik_osm_tags_filter)\n",
        "joint = IntersectionJoiner().transform(buffered_regions_gdf, features_gdf)\n",
        "hex2vec_embeddings = Hex2VecEmbedder(\n",
        "    expected_output_features=flat_geofabrik_osm_tags_filter,\n",
        "    encoder_sizes=[150, 100, 50, 16],\n",
        "    count_subcategories=True,\n",
        ").fit_transform(\n",
        "    buffered_regions_gdf,\n",
        "    features_gdf,\n",
        "    joint,\n",
        "    neighbourhood=H3Neighbourhood(buffered_regions_gdf),\n",
        "    batch_size=1024,\n",
        "    trainer_kwargs=dict(max_epochs=20),\n",
        ")\n",
        "features = listings_gdf.merge(\n",
        "    hex2vec_embeddings.reset_index(),\n",
        "    left_on=h3_embedding_column,\n",
        "    right_on=hex2vec_embeddings.index.name,\n",
        ").drop(\n",
        "    columns=[\n",
        "        target_column,\n",
        "        geometry_column,\n",
        "        h3_splitting_column,\n",
        "        h3_embedding_column,\n",
        "        hex2vec_embeddings.index.name,\n",
        "    ]\n",
        ")\n",
        "target = listings_gdf[target_column]\n",
        "\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NZBooTFATNl"
      },
      "outputs": [],
      "source": [
        "optuna_result = find_best_lightgbm_model(features, target, timeout=120)\n",
        "optuna_result[\"experiment\"] = \"hex2vec embeddings\"\n",
        "\n",
        "results.append(optuna_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TsKzFFexS96"
      },
      "source": [
        "## Display results on a map\n",
        "\n",
        "Now we will display the results for one of the models on the map - both predictions and relative errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK_aSl96zaAn"
      },
      "outputs": [],
      "source": [
        "aggregated_results = pd.DataFrame(results)[\n",
        "    [\"experiment\", \"n_features\", \"RMSE\", \"R2\", \"MAE\"]\n",
        "]\n",
        "aggregated_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TdyXj-TOayK"
      },
      "outputs": [],
      "source": [
        "# results from contextual count embeddings\n",
        "listings_predictions = listings_gdf.copy()\n",
        "\n",
        "listings_predictions[\"prediction\"] = results[-2][\"y_pred_all\"]\n",
        "\n",
        "# Symmetric mean absolute percentage error\n",
        "listings_predictions[\"smape\"] = (\n",
        "    2\n",
        "    * (listings_predictions[target_column] - listings_predictions[\"prediction\"])\n",
        "    / (listings_predictions[target_column] + listings_predictions[\"prediction\"])\n",
        ")\n",
        "\n",
        "listings_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oLj4W8jcnXI"
      },
      "outputs": [],
      "source": [
        "filtered_listings_gdf = listings_predictions[\n",
        "    listings_predictions[target_column] <= listings_predictions[target_column].quantile(0.95)\n",
        "][[geometry_column, target_column, \"prediction\"]]\n",
        "\n",
        "layer = ScatterplotLayer.from_geopandas(\n",
        "    filtered_listings_gdf,\n",
        "    get_fill_color=apply_continuous_cmap(\n",
        "        filtered_listings_gdf[\"prediction\"] / filtered_listings_gdf[\"prediction\"].max(),\n",
        "        mpl.colormaps[\"RdYlBu_r\"],\n",
        "        alpha=0.7,\n",
        "    ),\n",
        "    radius_units=\"meters\",\n",
        "    radius_min_pixels=1,\n",
        "    get_radius=filtered_listings_gdf[target_column] / 10,\n",
        ")\n",
        "\n",
        "m = Map(\n",
        "    layer, _height=800, basemap_style=lonboard.basemap.CartoBasemap.DarkMatterNoLabels\n",
        ")\n",
        "\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GP-P04rSyiU"
      },
      "outputs": [],
      "source": [
        "filtered_listings_gdf = listings_predictions[\n",
        "    listings_predictions[target_column] <= listings_predictions[target_column].quantile(0.95)\n",
        "].copy()\n",
        "\n",
        "filtered_listings_gdf[\"normalized_smape\"] = (\n",
        "    filtered_listings_gdf[\"smape\"].apply(\n",
        "        lambda x, filtered_listings_gdf=filtered_listings_gdf: (\n",
        "            -x / filtered_listings_gdf[\"smape\"].min()\n",
        "            if x < 0\n",
        "            else x / filtered_listings_gdf[\"smape\"].max()\n",
        "        )\n",
        "    )\n",
        "    + 1\n",
        ") / 2\n",
        "\n",
        "filtered_listings_gdf[\"normalized_smape_alpha\"] = (\n",
        "    filtered_listings_gdf[\"normalized_smape\"] - 0.5\n",
        ").abs() * 2\n",
        "\n",
        "layer = ScatterplotLayer.from_geopandas(\n",
        "    filtered_listings_gdf,\n",
        "    get_fill_color=apply_continuous_cmap(\n",
        "        filtered_listings_gdf[\"normalized_smape\"],\n",
        "        mpl.colormaps[\"bwr_r\"],\n",
        "        alpha=filtered_listings_gdf[\"normalized_smape_alpha\"],\n",
        "    ),\n",
        "    radius_units=\"meters\",\n",
        "    radius_min_pixels=1,\n",
        "    get_radius=filtered_listings_gdf[target_column] / 10,\n",
        ")\n",
        "\n",
        "h3_layer = PolygonLayer.from_geopandas(\n",
        "    gpd.GeoDataFrame(\n",
        "        geometry=h3_to_geoseries(\n",
        "            listings_gdf.loc[test_listing_ids, h3_splitting_column].unique()\n",
        "        )\n",
        "    ),\n",
        "    get_fill_color=[201, 152, 36, 15],\n",
        "    get_line_color=[201, 152, 36, 150],\n",
        "    get_line_width=20,\n",
        "    line_width_min_pixels=1,\n",
        "    line_width_max_pixels=10,\n",
        ")\n",
        "\n",
        "m = Map(\n",
        "    [h3_layer, layer],\n",
        "    _height=800,\n",
        "    basemap_style=lonboard.basemap.CartoBasemap.DarkMatterNoLabels,\n",
        ")\n",
        "\n",
        "m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task for you\n",
        "\n",
        "Easy task:\n",
        "- Add a new feature, showing a distance to the center of a city in meters in a straight line. (You can choose whatever you want as the city centre)\n",
        "- Train the new model with the embeddings of your choosing.\n",
        "- Compare the results of new model with previous results.\n",
        "\n",
        "Hard task:\n",
        "- Add a new feature, showing a distance to the nearest metro station in meters in a straight line.\n",
        "- Train the new model with the embeddings of your choosing.\n",
        "- Compare the results of new model with previous results.\n",
        "\n",
        "Those two values are present in the original data (columns `dist` and `metro_dist`, so you can compare your results later)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write code here\n",
        "\n",
        "# You can find answers here: https://github.com/kraina-ai/srai-tutorial/tree/geopython2025/answers"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
