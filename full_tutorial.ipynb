{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population in districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_population = pd.read_json('data/warsaw_population.json')\n",
    "warsaw_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_districts = gpd.read_file('data/warsaw_districts.geojson')\n",
    "warsaw_districts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_districts_with_population = warsaw_districts.merge(warsaw_population, on='district', how='inner')\n",
    "warsaw_districts_with_population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show data on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_districts_with_population.explore('population')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population in buildings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from OpenStreetMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.loaders.osm_loaders import OSMOnlineLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_region = warsaw_districts_with_population.unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = OSMOnlineLoader()\n",
    "\n",
    "osm_building_types = [\n",
    "    \"residential\",\n",
    "    \"apartments\",\n",
    "    \"house\",\n",
    "    \"semidetached_house\",\n",
    "    \"detached\",\n",
    "]\n",
    "\n",
    "buildings = loader.load(\n",
    "    gpd.GeoDataFrame(geometry=[warsaw_region], crs=\"EPSG:4326\"),\n",
    "    # warsaw_region,\n",
    "    {\"building\": osm_building_types, \"building:levels\": True},\n",
    ")\n",
    "buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings = buildings[buildings[\"building\"].isin(osm_building_types)]\n",
    "buildings = buildings.fillna(1)\n",
    "buildings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_floors(floors_value: str) -> int:\n",
    "    try:\n",
    "        floors = int(floors_value)\n",
    "    except Exception:\n",
    "        floors = 1\n",
    "\n",
    "    return floors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings[\"building:levels\"] = buildings[\"building:levels\"].map(map_floors)\n",
    "buildings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate population over buildings\n",
    "Use number of floors and area of a building as a weight - bigger and higher building usually have more inhabitants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings['weight'] = buildings.area * buildings['building:levels']\n",
    "buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 21.042753111534097\n",
    "xmax = 21.069257679735955\n",
    "ymin = 52.24187245384607\n",
    "ymax = 52.22533280016626\n",
    "buildings.cx[xmin:xmax, ymin:ymax].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings[\"geometry\"] = buildings.centroid\n",
    "buildings.cx[xmin:xmax, ymin:ymax].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_with_districts = buildings.sjoin(warsaw_districts_with_population, predicate='within')\n",
    "buildings_with_districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weight_in_districts = buildings_with_districts.groupby('district')['weight'].sum().rename(\"total_weight\")\n",
    "total_weight_in_districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_with_population = buildings_with_districts.merge(total_weight_in_districts, on=\"district\", how=\"inner\")\n",
    "buildings_with_population[\"population_in_building\"] = (\n",
    "    buildings_with_population[\"population\"] * buildings_with_population[\"weight\"] / buildings_with_population[\"total_weight\"]\n",
    ").round()\n",
    "buildings_with_population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate data into H3 index for easier representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_with_population[\"h3\"] = buildings_with_population.apply(\n",
    "    lambda row: h3.latlng_to_cell(row.geometry.y, row.geometry.x, 8), axis=1\n",
    ")\n",
    "buildings_with_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_aggregated = (\n",
    "    buildings_with_population.groupby(\"h3\")[\"population_in_building\"]\n",
    "    .sum()\n",
    "    .rename(\"population_in_h3\")\n",
    "    .reset_index()\n",
    ")\n",
    "population_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_aggregated.population_in_h3.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.h3 import h3_to_geoseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_aggregated = gpd.GeoDataFrame(population_aggregated, geometry=h3_to_geoseries(population_aggregated.h3))\n",
    "population_aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show data on the map\n",
    "3 proposed solutions:\n",
    "- simple GeoPandas `.explore()`\n",
    "- little bit prettier `srai.plotting` solution\n",
    "- 3D Deck.gl map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base folium - explore\n",
    "population_aggregated.explore('population_in_h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# srai - plot numeric\n",
    "from srai.plotting import plot_numeric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_data(\n",
    "    population_aggregated.rename(columns={\"h3\": \"region_id\"}).set_index(\"region_id\"),\n",
    "    \"population_in_h3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydeck 3d\n",
    "from srai.plotting.folium_wrapper import _generate_linear_colormap\n",
    "import plotly.express as px\n",
    "import pydeck as pdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = _generate_linear_colormap(\n",
    "    # https://plotly.com/python/builtin-colorscales/\n",
    "    px.colors.sequential.Aggrnyl_r,\n",
    "    min_value=population_aggregated[\"population_in_h3\"].min(),\n",
    "    max_value=population_aggregated[\"population_in_h3\"].max(),\n",
    ")\n",
    "\n",
    "population_aggregated[\"color\"] = population_aggregated[\"population_in_h3\"].map(\n",
    "    colormap.rgb_bytes_tuple\n",
    ")\n",
    "\n",
    "# Define a layer to display on a map\n",
    "layer = pdk.Layer(\n",
    "    \"H3HexagonLayer\",\n",
    "    population_aggregated,\n",
    "    pickable=True,\n",
    "    stroked=True,\n",
    "    filled=True,\n",
    "    extruded=True,\n",
    "    get_hexagon=\"h3\",\n",
    "    get_fill_color=\"[color[0], color[1], color[2], 204]\",\n",
    "    elevation_scale=0.5,\n",
    "    get_elevation=\"population_in_h3\",\n",
    "    coverage=0.8,\n",
    ")\n",
    "\n",
    "# Set the viewport location\n",
    "view_state = pdk.ViewState(\n",
    "    latitude=52.2317, longitude=21.0062, zoom=9.5, bearing=0, pitch=30\n",
    ")\n",
    "\n",
    "\n",
    "# Render\n",
    "pdk.Deck(\n",
    "    layers=[layer],\n",
    "    map_style=\"light\",\n",
    "    initial_view_state=view_state,\n",
    "    tooltip={\"text\": \"Population: {population_in_h3}\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneBoxes by Allegro\n",
    "Find out which one is the greenest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_boxes_raw = pd.read_json('data/oneboxes.json')\n",
    "one_boxes_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_boxes_gdf = gpd.GeoDataFrame(\n",
    "    one_boxes_raw,\n",
    "    geometry=gpd.GeoSeries.from_xy(one_boxes_raw[\"lon\"], one_boxes_raw[\"lat\"]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "one_boxes_gdf.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_one_boxes = one_boxes_gdf.clip(warsaw_region)\n",
    "warsaw_one_boxes.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.geometry import buffer_geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_one_boxes = warsaw_one_boxes.rename(columns={\"geometry\": \"point\"})\n",
    "warsaw_one_boxes[\"buffer_1000m\"] = warsaw_one_boxes[\"point\"].apply(\n",
    "    lambda geometry: buffer_geometry(geometry, 1000)\n",
    ")\n",
    "warsaw_one_boxes = warsaw_one_boxes.set_geometry(\"buffer_1000m\")\n",
    "warsaw_one_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_one_boxes.explore(style_kwds=dict(opacity=0.5, fillOpacity=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.loaders.osm_loaders import OSMPbfLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = OSMPbfLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greenery = loader.load(\n",
    "    gpd.GeoDataFrame(geometry=[warsaw_region], crs=\"EPSG:4326\"),\n",
    "    # warsaw_region,\n",
    "    {\n",
    "        \"leisure\": [\"garden\", \"park\"],\n",
    "        \"natural\": [\"wood\", \"scrub\", \"heath\", \"grassland\"],\n",
    "        \"landuse\": [\"grass\", \"orchard\", \"flowerbed\", \"forest\", \"greenfield\", \"meadow\"],\n",
    "    },\n",
    ")\n",
    "greenery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greenery = greenery[greenery.geom_type != 'Point']\n",
    "greenery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greenery.plot(color=\"tab:green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_one_boxes_with_greenery = warsaw_one_boxes.sjoin(greenery)\n",
    "warsaw_one_boxes_with_greenery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_one_boxes_with_greenery = gpd.overlay(\n",
    "    warsaw_one_boxes,\n",
    "    greenery,\n",
    "    how=\"intersection\",\n",
    ")\n",
    "\n",
    "warsaw_one_boxes_with_greenery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolve replaces groupby in spatial operations\n",
    "# https://geopandas.org/en/stable/docs/user_guide/aggregation_with_dissolve.html\n",
    "warsaw_one_boxes_with_greenery = warsaw_one_boxes_with_greenery.dissolve(by='id')\n",
    "warsaw_one_boxes_with_greenery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_one_boxes_with_greenery.plot(color='tab:green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_one_boxes_with_greenery['greenery_area'] = warsaw_one_boxes_with_greenery['geometry'].area\n",
    "warsaw_one_boxes_with_greenery = warsaw_one_boxes_with_greenery.sort_values(by='greenery_area', ascending=False)\n",
    "warsaw_one_boxes_with_greenery.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = warsaw_one_boxes.merge(\n",
    "    warsaw_one_boxes_with_greenery[\"greenery_area\"].reset_index(), on=\"id\"\n",
    ").explore(\n",
    "    \"greenery_area\",\n",
    "    cmap=\"BuGn\",\n",
    "    tiles=\"CartoDB positron\",\n",
    "    style_kwds=dict(opacity=0.5, fillOpacity=0.1),\n",
    ")\n",
    "\n",
    "warsaw_one_boxes['point'].explore(m=m, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warsaw_one_boxes_with_greenery.loc[[\"AL014WXY\"]].explore(color=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Venturilo - Contextual count embedder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_raw = pd.read_json('data/venturilo_stations.json')\n",
    "stations_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_gdf = gpd.GeoDataFrame(\n",
    "    stations_raw,\n",
    "    geometry=gpd.GeoSeries.from_xy(stations_raw[\"lon\"], stations_raw[\"lat\"]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "stations_gdf.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.regionalizers import H3Regionalizer\n",
    "from srai.plotting import plot_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_regions = H3Regionalizer(resolution=9).transform(\n",
    "    gpd.GeoDataFrame(geometry=[warsaw_region], crs=\"EPSG:4326\")\n",
    "    # warsaw_region\n",
    ")\n",
    "plot_regions(h3_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.loaders.osm_loaders.filters import GEOFABRIK_LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geofabrik_features = OSMPbfLoader().load(\n",
    "    gpd.GeoDataFrame(geometry=[warsaw_region], crs=\"EPSG:4326\"),\n",
    "    # warsaw_region,\n",
    "    GEOFABRIK_LAYERS\n",
    ")\n",
    "geofabrik_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geofabrik_features_without_stations = geofabrik_features[\n",
    "    geofabrik_features[\"shopping\"] != \"amenity=bicycle_rental\"\n",
    "]\n",
    "geofabrik_features_without_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.embedders import ContextualCountEmbedder\n",
    "from srai.neighbourhoods import H3Neighbourhood\n",
    "from srai.joiners import IntersectionJoiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_features = IntersectionJoiner().transform(h3_regions, geofabrik_features_without_stations)\n",
    "joined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = ContextualCountEmbedder(neighbourhood=H3Neighbourhood(), neighbourhood_distance=5, concatenate_vectors=True)\n",
    "embeddings = embedder.transform(regions_gdf=h3_regions, features_gdf=geofabrik_features_without_stations, joint_gdf=joined_features)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_joint = IntersectionJoiner().transform(h3_regions, stations_gdf)\n",
    "bikes_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = h3_regions.join(bikes_joint, how=\"inner\")\n",
    "positive_samples = positive_samples.reset_index().drop(columns=[\"feature_id\"]).set_index(\"region_id\")\n",
    "positive_samples[\"is_positive\"] = True\n",
    "len(positive_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples = h3_regions.copy()\n",
    "negative_samples[\"is_positive\"] = False\n",
    "negative_samples.loc[positive_samples.index, \"is_positive\"] = True\n",
    "negative_samples = negative_samples[~negative_samples[\"is_positive\"]]\n",
    "len(negative_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_undersampled = negative_samples.sample(n=3 * len(positive_samples), random_state=42)\n",
    "negative_undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_SAFE_PALLETE = [\n",
    "    \"#377eb8\",\n",
    "    \"#ff7f00\",\n",
    "    \"#4daf4a\",\n",
    "    \"#f781bf\",\n",
    "    \"#a65628\",\n",
    "    \"#984ea3\",\n",
    "    \"#999999\",\n",
    "    \"#e41a1c\",\n",
    "    \"#dede00\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([positive_samples, negative_undersampled])\n",
    "train_data.explore(\"is_positive\", cmap=CB_SAFE_PALLETE, zoom_start=14, height=600, tiles=\"CartoDB positron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = embeddings.loc[train_data.index].to_numpy()\n",
    "y = train_data[\"is_positive\"].astype(int).to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(probability=True)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred_proba = classifier.predict_proba(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_probas = classifier.predict_proba(embeddings.to_numpy())\n",
    "\n",
    "h3_regions[\"station_proba\"] = station_probas[:, 1]\n",
    "m = plot_numeric_data(h3_regions, \"station_proba\", colormap=\"Spectral_r\", opacity=0.5)\n",
    "\n",
    "stations_gdf.explore(m=m, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.regionalizers import geocode_to_region_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wroclaw_region = geocode_to_region_gdf('Wrocław, PL')\n",
    "wroclaw_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wroclaw_h3_regions = H3Regionalizer(resolution=9).transform(wroclaw_region)\n",
    "wroclaw_h3_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wroclaw_geofabrik_features = OSMPbfLoader().load(\n",
    "    wroclaw_region,\n",
    "    GEOFABRIK_LAYERS\n",
    ")\n",
    "wroclaw_geofabrik_features_without_stations = wroclaw_geofabrik_features[\n",
    "    wroclaw_geofabrik_features[\"shopping\"] != \"amenity=bicycle_rental\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wroclaw_joined_features = IntersectionJoiner().transform(wroclaw_h3_regions, wroclaw_geofabrik_features_without_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wroclaw_embeddings = embedder.transform(\n",
    "    regions_gdf=wroclaw_h3_regions,\n",
    "    features_gdf=wroclaw_geofabrik_features_without_stations,\n",
    "    joint_gdf=wroclaw_joined_features,\n",
    ")\n",
    "wroclaw_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_probas = classifier.predict_proba(wroclaw_embeddings.to_numpy())\n",
    "\n",
    "wroclaw_h3_regions[\"station_proba\"] = station_probas[:, 1]\n",
    "plot_numeric_data(wroclaw_h3_regions, \"station_proba\", colormap=\"Spectral_r\", opacity=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxis and flow prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('data/trips_hexes.zip', \"r\") as zf:\n",
    "    for member in tqdm(zf.infolist(), desc=\"\"):\n",
    "        try:\n",
    "            zf.extract(member, 'data')\n",
    "        except zipfile.error:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_trips_h3 = pd.read_csv('data/trips_hexes.csv')\n",
    "taxi_trips_h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_trips_h3['start_point'] = taxi_trips_h3['start_hex'].apply(h3.cell_to_latlng)\n",
    "taxi_trips_h3['end_point'] = taxi_trips_h3['end_hex'].apply(h3.cell_to_latlng)\n",
    "taxi_trips_h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_trips_h3['start_lat'], taxi_trips_h3['start_lon'] = zip(*taxi_trips_h3['start_point'])\n",
    "taxi_trips_h3['end_lat'], taxi_trips_h3['end_lon'] = zip(*taxi_trips_h3['end_point'])\n",
    "taxi_trips_h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_trips_h3[\"trips_normalized\"] = (\n",
    "    (taxi_trips_h3[\"trips\"] - taxi_trips_h3[\"trips\"].min())\n",
    "    / (taxi_trips_h3[\"trips\"].max() - taxi_trips_h3[\"trips\"].min())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_layer = pdk.Layer(\n",
    "    \"ArcLayer\",\n",
    "    data=taxi_trips_h3.sample(frac=0.1),\n",
    "    get_width=\"0.5 + trips_normalized * 9\",\n",
    "    get_source_position=[\"start_lon\", \"start_lat\"],\n",
    "    get_target_position=[\"end_lon\", \"end_lat\"],\n",
    "    get_tilt=15,\n",
    "    get_source_color=\"[0, 255, 0, 40 + trips_normalized * 215]\",\n",
    "    get_target_color=\"[0, 150, 255, 40 + trips_normalized * 215]\",\n",
    "    pickable=True,\n",
    "    auto_highlight=True,\n",
    ")\n",
    "\n",
    "view_state = pdk.ViewState(latitude=41.1493, longitude=-8.6111, bearing=45, pitch=65, zoom=10.5,)\n",
    "\n",
    "TOOLTIP_TEXT = {\"html\": \"{trips} trips <br /> Start of the trip in green; end of the trip in blue\"}\n",
    "pdk.Deck(arc_layer, initial_view_state=view_state, tooltip=TOOLTIP_TEXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hexes = set(taxi_trips_h3['start_hex'].unique()).union(taxi_trips_h3['end_hex'].unique())\n",
    "len(unique_hexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = [h3.cell_to_latlng(h3_cell)[::-1] for h3_cell in unique_hexes]\n",
    "coordinates[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_points = gpd.GeoDataFrame(geometry=gpd.GeoSeries.from_xy(*zip(*coordinates)), crs='EPSG:4326')\n",
    "unique_points.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.regionalizers import AdministrativeBoundaryRegionalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_regionalizer = AdministrativeBoundaryRegionalizer(admin_level=7, clip_regions=False)\n",
    "\n",
    "municipalities = portugal_regionalizer.transform(unique_points)\n",
    "municipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "municipalities.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_h3_resolution = h3.get_resolution(taxi_trips_h3['start_hex'].iloc[0])\n",
    "trip_h3_resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_h3_regions = H3Regionalizer(resolution=trip_h3_resolution).transform(municipalities)\n",
    "portugal_h3_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = OSMPbfLoader()\n",
    "\n",
    "portugal_features = loader.load(municipalities, GEOFABRIK_LAYERS)\n",
    "portugal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_features[portugal_features.geom_type != 'Point'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_joint_features = IntersectionJoiner().transform(portugal_h3_regions, portugal_features)\n",
    "portugal_joint_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srai.embedders import Hex2VecEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex2vec_embedder = Hex2VecEmbedder(encoder_sizes=[300, 150, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_h3_neighbourhood = H3Neighbourhood(portugal_h3_regions)\n",
    "portugal_embeddings = hex2vec_embedder.fit_transform(\n",
    "    regions_gdf=portugal_h3_regions,\n",
    "    features_gdf=portugal_features,\n",
    "    joint_gdf=portugal_joint_features,\n",
    "    neighbourhood=portugal_h3_neighbourhood,\n",
    "    negative_sample_k_distance=2,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.001,\n",
    "    trainer_kwargs={\n",
    "        # \"max_epochs\": 50, # uncomment for a longer training\n",
    "        \"max_epochs\": 5,\n",
    "        # \"accelerator\": \"cpu\",\n",
    "    },\n",
    ")\n",
    "portugal_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "pca_embeddings = pca.fit_transform(portugal_embeddings)\n",
    "# make the embeddings into a dataframe\n",
    "pca_embeddings = pd.DataFrame(pca_embeddings, index=portugal_embeddings.index)\n",
    "\n",
    "# convert to RGB\n",
    "pca_embeddings = (\n",
    "    (pca_embeddings - pca_embeddings.min())\n",
    "    / (pca_embeddings.max() - pca_embeddings.min())\n",
    "    * 255\n",
    ").astype(int)\n",
    "\n",
    "# make the rgb array into a string\n",
    "pca_embeddings[\"rgb\"] = pca_embeddings.apply(\n",
    "    lambda row: f\"rgb({row[0]}, {row[1]}, {row[2]})\", axis=1\n",
    ")\n",
    "\n",
    "porto_regions = portugal_h3_regions[\n",
    "    portugal_h3_regions.intersects(\n",
    "        municipalities.loc[[\"Porto\", \"Vila Nova de Gaia\"]].unary_union\n",
    "    )\n",
    "]\n",
    "\n",
    "color_dict = dict(\n",
    "    enumerate(porto_regions.index.map(pca_embeddings[\"rgb\"].to_dict()).to_list())\n",
    ")\n",
    "porto_regions.reset_index().reset_index().explore(\n",
    "    column=\"index\",\n",
    "    tooltip=\"region_id\",\n",
    "    tiles=\"CartoDB positron\",\n",
    "    legend=False,\n",
    "    cmap=lambda x: color_dict[x],\n",
    "    style_kwds=dict(color=\"#444\", opacity=0.0, fillOpacity=0.5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_h3_index = portugal_embeddings.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_trips_per_hex = taxi_trips_h3.groupby('start_hex')['trips'].sum()\n",
    "sum_of_trips_per_hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_h3_index_with_trips = portugal_h3_index.intersection(sum_of_trips_per_hex.index)\n",
    "portugal_h3_index_with_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_h3_index_without_trips = portugal_h3_index.difference(sum_of_trips_per_hex.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_trips_per_hex_full = pd.DataFrame(\n",
    "    data=dict(\n",
    "        trips=[0 for _ in portugal_h3_index_without_trips] + list(sum_of_trips_per_hex.values)\n",
    "    ),\n",
    "    index=list(portugal_h3_index_without_trips) + list(sum_of_trips_per_hex.index),\n",
    ")\n",
    "sum_of_trips_per_hex_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = portugal_embeddings.loc[sum_of_trips_per_hex_full.index].to_numpy()\n",
    "y = sum_of_trips_per_hex_full[\"trips\"].astype(int).to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = SVR()\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    r2_score(y_test, y_pred),\n",
    "    mean_absolute_error(y_test, y_pred),\n",
    "    mean_absolute_percentage_error(y_test, y_pred),\n",
    ")\n",
    "# Those numbers are probably very bad xD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add predicted number of trips to the visualization with some splitting over destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "def weighted_mse_loss(input, target, weight):\n",
    "    return torch.sum(weight * (input - target) ** 2)\n",
    "\n",
    "class TripPredictorModel(LightningModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.nn_model = nn.Sequential(\n",
    "            nn.Linear(50, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(30, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(30, 50),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: \"torch.Tensor\") -> \"torch.Tensor\":\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "        \"\"\"\n",
    "        embedding: \"torch.Tensor\" = self.nn_model(x)\n",
    "        return embedding\n",
    "\n",
    "    def configure_optimizers(self) -> \"torch.optim.Optimizer\":\n",
    "        \"\"\"Configure optimizer.\"\"\"\n",
    "        import torch\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch: List[\"torch.Tensor\"], batch_idx: Any) -> \"torch.Tensor\":\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "\n",
    "        Args:\n",
    "            batch (torch.Tensor): Batch.\n",
    "            batch_idx (Any): Batch index.\n",
    "        \"\"\"\n",
    "        x, y, weight = batch\n",
    "        y_pred = self.nn_model(x)\n",
    "        loss = weighted_mse_loss(y_pred, y, weight)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TripsDataset(Dataset):\n",
    "    def __init__(self, start_embeddings, end_embeddings, trips):\n",
    "        self.start_embeddings = torch.Tensor(start_embeddings)\n",
    "        self.end_embeddings = torch.Tensor(end_embeddings)\n",
    "        self.trips = torch.Tensor(trips.reshape((len(trips), 1)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.start_embeddings[index]\n",
    "        y = self.end_embeddings[index]\n",
    "        weight = self.trips[index]\n",
    "        return x, y, weight\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.start_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_hexes = taxi_trips_h3['start_hex']\n",
    "end_hexes = taxi_trips_h3['end_hex']\n",
    "no_trips = taxi_trips_h3['trips']\n",
    "\n",
    "trips_dataset = TripsDataset(\n",
    "    start_embeddings=portugal_embeddings.loc[start_hexes].to_numpy(),\n",
    "    end_embeddings=portugal_embeddings.loc[end_hexes].to_numpy(),\n",
    "    trips=no_trips.to_numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer_kwargs = {\n",
    "    # \"max_epochs\": 50, # uncomment for a longer training\n",
    "    \"max_epochs\": 5,\n",
    "    # \"accelerator\": \"cpu\",\n",
    "}\n",
    "\n",
    "dataloader = DataLoader(trips_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "trip_predictor_model = TripPredictorModel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(**trainer_kwargs)\n",
    "trainer.fit(trip_predictor_model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_annoy_index = AnnoyIndex(50, \"angular\")\n",
    "\n",
    "all_portugal_embeddings = portugal_embeddings.to_numpy()\n",
    "all_portugal_embeddings.shape\n",
    "\n",
    "for idx in tqdm(range(len(portugal_h3_index)), total=len(portugal_h3_index)):\n",
    "    portugal_annoy_index.add_item(idx, all_portugal_embeddings[idx])\n",
    "    \n",
    "portugal_annoy_index.build(100, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_predicted_trips_embeddings = trip_predictor_model(torch.Tensor(all_portugal_embeddings)).detach().numpy()\n",
    "portugal_predicted_trips_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_pairs = []\n",
    "for idx in tqdm(range(len(portugal_h3_index)), total=len(portugal_h3_index)):\n",
    "    trip_end_embedding = portugal_predicted_trips_embeddings[idx]\n",
    "    nearest_neighbours_ids, distances = portugal_annoy_index.get_nns_by_vector(\n",
    "        # trip_end_embedding, n=100, include_distances=True\n",
    "        trip_end_embedding,\n",
    "        n=10,\n",
    "        include_distances=True,\n",
    "    )\n",
    "    nearest_neighbours = portugal_h3_index[nearest_neighbours_ids]\n",
    "\n",
    "    start_index = portugal_h3_index[idx]\n",
    "    for nearest_neighbour, distance in zip(nearest_neighbours, distances):\n",
    "        trip_pairs.append(\n",
    "            dict(start_hex=start_index, end_hex=nearest_neighbour, distance=distance)\n",
    "        )\n",
    "\n",
    "predicted_trips = pd.DataFrame(trip_pairs)\n",
    "predicted_trips[\"start_point\"] = predicted_trips[\"start_hex\"].apply(h3.cell_to_latlng)\n",
    "predicted_trips[\"end_point\"] = predicted_trips[\"end_hex\"].apply(h3.cell_to_latlng)\n",
    "predicted_trips[\"start_lat\"], predicted_trips[\"start_lon\"] = zip(\n",
    "    *predicted_trips[\"start_point\"]\n",
    ")\n",
    "predicted_trips[\"end_lat\"], predicted_trips[\"end_lon\"] = zip(\n",
    "    *predicted_trips[\"end_point\"]\n",
    ")\n",
    "predicted_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_layer = pdk.Layer(\n",
    "    \"ArcLayer\",\n",
    "    data=predicted_trips.sample(frac=0.1),\n",
    "    # get_width=\"5 * (1 - distance)\",\n",
    "    get_width=\"0.5 * (1 - distance)\",\n",
    "    get_source_position=[\"start_lon\", \"start_lat\"],\n",
    "    get_target_position=[\"end_lon\", \"end_lat\"],\n",
    "    get_tilt=15,\n",
    "    get_source_color=\"[0, 255, 0, 40 + trips_normalized * 215]\",\n",
    "    get_target_color=\"[0, 150, 255, 40 + trips_normalized * 215]\",\n",
    "    pickable=True,\n",
    "    auto_highlight=True,\n",
    ")\n",
    "\n",
    "view_state = pdk.ViewState(latitude=41.1493, longitude=-8.6111, bearing=45, pitch=65, zoom=10.5,)\n",
    "\n",
    "TOOLTIP_TEXT = {\"html\": \"Predicted distance {distance} <br /> Start of the trip in green; end of the trip in blue\"}\n",
    "pdk.Deck(arc_layer, initial_view_state=view_state, tooltip=TOOLTIP_TEXT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
